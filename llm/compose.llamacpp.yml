services:
  llamacpp:
    image: ghcr.io/ggml-org/llama.cpp:server
    container_name: llamacpp
    restart: unless-stopped
    ports:
      - ${LLAMA_ARG_PORT:-8080}:${LLAMA_ARG_PORT:-8080}
    volumes:
      - ${DOCKER_VOLUME_STORAGE:-../data}/llamacpp:/models
      - ${DOCKER_VOLUME_STORAGE:-../data}/llamacpp:/root/.cache/llama.cpp/
    env_file:
      - path: ../.env
        required: true